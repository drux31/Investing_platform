--> airflow tutorial with docker
https://airflow.apache.org/docs/apache-airflow/stable/tutorial/pipeline.html

- Airflow :
Dowload the yaml file :
    >curl -LfO 'https://airflow.apache.org/docs/apache-airflow/2.7.2/docker-compose.yaml'

setup the environment (Linux):
    >mkdir -p ./dags ./logs ./plugins ./config
    >echo -e "AIRFLOW_UID=$(id -u)" > .env

Initialize the database:
    > docker-compose up airflow-init

Run aiflow :
    > docker-compose up -d


creating a dataframe from a list :
-> Just fill the list first, then create a dataframe from the list.
https://stackoverflow.com/questions/13784192/creating-an-empty-pandas-dataframe-and-then-filling-it

Add google credential to the yaml file so docker has the informations.
GOOGLE_APPLICATION_CREDENTIALS: /.google/credentials/google_credentials.json
AIRFLOW_CONN_GOOGLE_CLOUD_DEFAULT: 'google-cloud-platform://?extra__google_cloud_platform__key_path=/.google/credentials/google_credentials.json'

copy the directory from local to docker :
- ~/.google/credentials/:/.google/credentials:ro

in the python file :
import the operator to use :
- example for gcs : from airflow.providers.google.cloud.transfers.local_to_gcs import LocalFilesystemToGCSOperator

In order to configure BigQuery from a python function, use the following :
https://googleapis.dev/python/google-api-core/latest/auth.html
https://codelabs.developers.google.com/codelabs/cloud-bigquery-python#0


